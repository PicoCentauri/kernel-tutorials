{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook, we introduce kernel-based versions of the models covered in the [Linear Methods](1_LinearMethods.ipynb) and [PCovR](2_PrincipalCovariatesRegression.ipynb) notebooks.\n",
    "\n",
    "As always, for each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. Second, we will employ a \"Sklearn/Skcosmo Class\" for the models, which can be found in the skcosmo/sklearn mudule and contains all necessary functions.\n",
    "\n",
    "**Note**: To display values of variables in the markdown cells, you should [enable the jupyter contrib nbextension](https://stackoverflow.com/questions/52812231/print-variable-in-jupyter-notebook-markdown-cell-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append(\"../\")\n",
    "from utilities.general import load_variables, sorted_eig, get_stats\n",
    "from utilities.plotting import (\n",
    "    plot_projection,\n",
    "    plot_regression,\n",
    "    check_mirrors,\n",
    "    get_cmaps,\n",
    "    table_from_dict,\n",
    ")\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from functools import partial\n",
    "\n",
    "from skcosmo.decomposition import KernelPCovR\n",
    "from skcosmo.preprocessing import KernelNormalizer\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use(\"../utilities/kernel_pcovr.mplstyle\")\n",
    "dbl_fig = (2 * plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb). As mentioned in the [foreword notebook](0_Foreword.ipynb), pre-computed features should be downloaded from [here](https://www.dropbox.com/s/itokckbbkvxaqsk/precomputed.npz?dl=0) and the file `precomputed.npz` should be copied to the `datasets/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Kernels\n",
    "\n",
    "## The Kernel Trick\n",
    "\n",
    "Many kernel methods are similar to linear methods,\n",
    "except that we take advantage of the [**kernel trick**](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick) in order to \n",
    "introduce a non-linear transformation of the feature space.\n",
    "\n",
    "The kernel trick consists in introducing a [**positive definite**](https://en.wikipedia.org/wiki/Positive-definite_kernel) function of pairs of samples, $ k(\\mathbf{x}, \\mathbf{x}^{\\prime})$, that defines implicitly a higher (possibly infinite) dimensional feature space, in which one can apply linear analysis methods. \n",
    "\n",
    "There are [many](https://en.wikipedia.org/wiki/Positive-definite_kernel#Examples_of_p.d._kernels) positive definite kernels. Common kernels are the linear (dot product) kernel,\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\mathbf{x}^T \\mathbf{x},\n",
    "\\end{equation}\n",
    "and the Gaussian kernel,\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\exp{\\left(-\\gamma\\lVert \\mathbf{x} - \\mathbf{x}^{\\prime}\\rVert^2\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "In this notebook we use the gaussian kernel (```rbf_kernel(XA, XB, gamma=1.0)```), however, we have also included the linear kernel function (```linear_kernel(XA, XB)```) in ```sklearn```. You can check rather easily that if you use a linear kernel all of the kernel methods reduce explicitly to linear regression or principal component analysis in the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the kernels in this cell will change them throughout the notebook tutorial.\n",
    "# The function variable designates the kernel function to use throughout the notebook.\n",
    "# The string variable is used to pass to the utility classes.\n",
    "kernel_params = {\"kernel\": \"rbf\", \"gamma\": 1.0}\n",
    "kernel_func = partial(rbf_kernel, gamma=1.0)\n",
    "kernel_type = \"gaussian\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [representer theorem](https://en.wikipedia.org/wiki/Representer_theorem) guarantees that if the kernel is a positive definite function (both these two examples are) there is a Hilbert space with elements $\\phi(\\mathbf{x})$ whose dot product reproduces the kernel, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\phi(\\mathbf{x})^T\\phi(\\mathbf{x}^{\\prime}).\n",
    "\\end{equation}\n",
    "\n",
    "This Hilbert space is known as the [**reproducing kernel Hilbert space**](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space), or RKHS. If one builds a kernel matrix $\\mathbf{K}$ whose elements contain the kernel computed between the corresponding pair of samples, \n",
    "For simple kernels, such as the linear kernel shown above, the RKHS is intuitive: $\\phi(\\mathbf{x}) = \\mathbf{x}$.\n",
    "However, for even slightly more complex kernel functions, the RKHS may be infinitely dimensional, such as the [RBF kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel).\n",
    "If one builds a kernel matrix $\\mathbf{K}$ whose elements contain the kernel computed between the corresponding pair of samples, \n",
    "\n",
    "\\begin{equation}\n",
    "    K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)\n",
    "\\end{equation}\n",
    "\n",
    "the formal relation with the reproducing features can be written in a matrix notation\n",
    "\n",
    "\\begin{equation}\n",
    "\\quad \\mathbf{K} = \\mathbf{\\Phi_X\\Phi_X^T}.\n",
    "\\end{equation}\n",
    "\n",
    "$\\mathbf{\\Phi_X}$ indicates a matrix that holds the values of the RKHS features for each sample in $\\mathbf{X}$, which means that $\\mathbf{X}$and $\\mathbf{\\Phi_X}$ have the same number of samples, but different numbers of features. To simplify the notation, in this notebook we drop the $\\mathbf{X}$ subscript and refer to this matrix simply as $\\mathbf{\\Phi}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train_raw = kernel_func(X_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize that even if in general $\\phi(\\mathbf{x})$ is not known, _for a given data set_ the representer theorem provides an explicit construction for an approximation of the RKHS features. The construction is very closely related to the KPCA method that we discuss below. \n",
    "\n",
    "Start by writing an eigendecomposition of the kernel matrix, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} = \\mathbf{U_K} \\mathbf{\\Lambda_K} \\mathbf{U_K}^T.\n",
    "\\end{equation}\n",
    "\n",
    "If one defines \n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi} =  \\mathbf{U_K} \\mathbf{\\Lambda_K}^{1/2} = \\mathbf{K} \\mathbf{U_K} \\mathbf{\\Lambda_K}^{-1/2}\n",
    "\\end{equation}\n",
    "one sees that $\\mathbf{\\Phi}\\mathbf{\\Phi}^T = \\mathbf{K}$: the eigenvectors of the kernel matrix make it possible to construct a set of features whose scalar product reproduces exactly the values of the kernel for the dataset. \n",
    "\n",
    "The second equality is important because it provides a recipe to build an _approximate_ set of RKHS features for a _new_ set of points. If $\\mathbf{K}_{NN}$ indicates the kernel matrix for the train set, and the matrix $\\mathbf{K}_{N'N}$ contains the kernels between some new (e.g. test-set) samples and the train samples, which means the size of $\\mathbf{K}_{N'N}$  is $n_{new\\ samples}\\times n_{train\\ sample}$,  one can compute \n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi}_{N'N} = \\mathbf{K}_{N'N} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2},\n",
    "\\end{equation}\n",
    "where $\\mathbf{U}$ and $\\mathbf{\\Lambda}$ refer to the eigendecomposition of the square $\\mathbf{K}_{NN}$.\n",
    "Then, $\\mathbf{\\Phi}_{N'N}$ is a matrix whose entries approximate the RKHS features for the new set of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_K, U_K = np.linalg.eigh(K_train_raw)\n",
    "\n",
    "# U_K/v_K are already sorted, but in *increasing* order, so reverse them\n",
    "U_K = np.flip(U_K, axis=1)\n",
    "v_K = np.flip(v_K, axis=0)\n",
    "\n",
    "U_K = U_K[:, v_K > 0]\n",
    "v_K = v_K[v_K > 0]\n",
    "\n",
    "Phi = U_K @ np.diag(np.sqrt(v_K))\n",
    "\n",
    "print(np.linalg.norm((K_train_raw - Phi @ Phi.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Centering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as it is often convenient to center features for linear methods, it is often beneficial to work with kernels that are also \"centered\". Centering a kernel can be understood in terms of centering of the associated RKHS features.  \n",
    "\n",
    "Let us first introduce the centering matrix $\\mathbf{1}_{N'N}$, which is just is a $N'\\times N$ matrix for which each element takes value $1/N$. \n",
    "\n",
    "Using the expression above for $\\mathbf{\\Phi}$, one can compute \n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{\\mathbf{\\Phi}}_{N'N} = \\mathbf{1}_{N'N} \\mathbf{K}_{NN} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "A similar centering matrix can be built for $\\mathbf{\\Phi}_{NN}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{\\mathbf{\\Phi}}_{NN} = \\mathbf{1}_{NN} \\mathbf{K}_{NN} \\mathbf{U} \\mathbf{\\Lambda}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "The centered kernel $\\tilde{\\mathbf{K}}_{N'N}$ reads\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{K}}_{N'N} = (\\mathbf{\\Phi}_{N'N} - \\bar{\\mathbf{\\Phi}}_{N'N})(\\mathbf{\\Phi}_{NN}-\\bar{\\mathbf{\\Phi}}_{NN})^T = \n",
    "\\mathbf{K}_{N'N} -  \\mathbf{1}_{N'N} \\mathbf{K}_{NN} -  \\mathbf{K}_{N'N}\\mathbf{1}_{NN}\n",
    "+ \\mathbf{1}_{N'N} \\mathbf{K}_{NN} \\mathbf{1}_{NN}.\n",
    "\\end{equation}\n",
    "\n",
    "which can be computed without the need of ever evaluating explicitly the RKHS features.\n",
    "This is a common pattern in kernel methods: RKHS features allow casting problems in a linear\n",
    "language, but eventually the linear problem can yield an equation in which one only needs to evaluate the kernel function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we center the kernels, which can be done with a utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_centerer = KernelNormalizer()\n",
    "K_train = k_centerer.fit_transform(K_train_raw)\n",
    "K_scale = k_centerer.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Testing Set Kernel\n",
    "\n",
    "For new data, we must generate a new kernel. Notice that a kernel takes *two* sets of data as arguments, one of which can be our testing $\\mathbf{X}_{N'}$ and the other the training $\\mathbf{X}_N$. This contains the kernels computed between all of the test set samples $N'$ and the train set samples $N$. This can be computed as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_test = kernel_func(X_test, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We center with the training kernel as reference, as discussed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centering relative to the approximate RHKS defined by\n",
    "# the training kernel matrix can be achieved specifying\n",
    "# K_train as the reference kernel matrix\n",
    "\n",
    "K_test = k_centerer.transform(K_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA (KPCA)\n",
    "\n",
    "In [kernel principal component analysis](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)\n",
    "(KPCA) we take advantage of the kernel in order to \n",
    "introduce a non-linear transformation of the feature space \n",
    "[(Scholkopf 1996)](http://www.face-rec.org/algorithms/Kernel/kernelPCA_scholkopf.pdf), \n",
    "[(Scholkopf 1998)](http://www.doi.org/10.1162/089976698300017467), and then proceed to single out the largest-variance directions in RKHS. \n",
    "\n",
    "With the linear (dot product) kernel, performing KPCA with a linear kernel is equivalent to performing standard PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principal Components\n",
    "\n",
    "KPCA proceeds analogously to PCA. First, the eigenvalues and eigenvectors of $\\mathbf{K}$ are computed:\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} = \\mathbf{U_K} \\mathbf{\\Lambda_K} \\mathbf{U_K}^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_K, U_K = np.linalg.eigh(K_train)\n",
    "\n",
    "# U_K/v_K are already sorted, but in *increasing* order, so reverse them\n",
    "U_K = np.flip(U_K, axis=1)\n",
    "v_K = np.flip(v_K, axis=0)\n",
    "\n",
    "U_K = U_K[:, v_K > 0]\n",
    "v_K = v_K[v_K > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The KPCA Projection\n",
    "\n",
    "The KPCA projection may then be computed taking the first $n_{PCA}$ components \n",
    "\\begin{equation}\n",
    "\\mathbf{T}=\\mathbf{\\hat{U}_K}\\mathbf{\\hat{\\Lambda}_K}^{1/2} = \\mathbf{\\hat{U}_K}\\mathbf{\\hat{\\Lambda}_K} (\\mathbf{\\hat{U}_K}^T\\mathbf{\\hat{U}_K}) \\mathbf{\\hat{\\Lambda}_K}^{-1/2} = \\mathbf{K} \\mathbf{\\hat{U}_K} \\mathbf{\\hat{\\Lambda}_K}^{-1/2}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\mathbf{T} = \\mathbf{K}\\mathbf{P}_{KT} = \\mathbf{K} \\mathbf{\\hat{U}_K} \\mathbf{\\hat{\\Lambda}_K}^{-1/2}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember that in PCA, the projection $\\mathbf{T} = \\mathbf{X}\\mathbf{U}_C$. So why the factor of $\\mathbf{\\Lambda}_K^{-1/2}$?**\n",
    "\n",
    "The eigenvectors of $\\mathbf{K} = \\mathbf{\\Phi}\\mathbf{\\Phi}^T$ are in fact analogous to the eigenvectors of the Gram matrix $\\mathbf{X}\\mathbf{X}^T$ and thereby related to those of the covariance $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X} = \\mathbf{U}_C \\mathbf{\\Lambda}_C \\mathbf{U}_C^T$ by $\\mathbf{X}\\mathbf{U_C}$, which is not normalized. In essence, the $\\mathbf{\\Lambda}_K^{-1/2}$ factor serves to normalize our projection matrix. [(Tipping 2001)](https://papers.nips.cc/paper/1791-sparse-kernel-principal-component-analysis.pdf) \n",
    "\n",
    "Note also that the KPCA latent space corresponds to the highest-variance components in the RKHS: if one does not truncate the latent space, $\\mathbf{T}=\\mathbf{\\Phi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_PC = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = U_K[:, :n_PC] @ np.diagflat(1.0 / np.sqrt(v_K[0:n_PC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for the testing data is then identical to that for the projection of the train set:\n",
    "\\begin{equation}\n",
    "    \\mathbf{T} = \\mathbf{K}_{N'N} \\mathbf{P}_{KT}\n",
    "\\end{equation}\n",
    "$\\mathbf{P}_{KT}$ is the projection obtained during training, while the $\\mathbf{K}_{N'N}$ indicates the kernel between the test set and the training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KPCA_train = K_train @ PKT\n",
    "T_KPCA_test = K_test @ PKT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: if we used a linear kernel, the projection would be identical to the PCA projection (modulo reflection, since the sign of the eigenvectors is not defined), and it would likewise correspond to the Classical MDS solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "plot_projection(\n",
    "    Y_train, T_KPCA_train, fig=fig, ax=axes[0], title=\"KPCA of Training Data\", **cmaps\n",
    ")\n",
    "plot_projection(\n",
    "    Y_test, T_KPCA_test, fig=fig, ax=axes[1], title=\"KPCA of Testing Data\", **cmaps\n",
    ")\n",
    "fig.subplots_adjust(wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is not done very often (because the kernel acts in a different space than the original feature space) it is possible to reconstruct an approximation of the feature vector based on the KPCA latent space. Consider this like \"predicting\" $\\mathbf{X}$, similar to predicting $\\mathbf{Y}$ in linear regression. The projection matrix $\\mathbf{P}_{TX}$ corresponds to the least-square weights\n",
    "    \n",
    "\\begin{equation}    \n",
    "\\mathbf{P}_{TX} = (\\mathbf{T}\\mathbf{T}^T)^{-1}\\mathbf{T}^T \\mathbf{X} =  \\mathbf{\\hat{\\Lambda}}_K^{-1} \\mathbf{T}^T \\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "Where the factor of $\\mathbf{\\Lambda_K}^{-1}$ arises from the fact that\n",
    "$\\mathbf{T}^T\\mathbf{T}=\n",
    "\\mathbf{\\Lambda_K}^{-1/2}\\mathbf{U_K}^T\\mathbf{K}^T\\mathbf{K}\\mathbf{U_K}\\mathbf{\\Lambda_K}^{-1/2}=\n",
    "\\mathbf{\\Lambda_K}^{1/2}\\mathbf{U_K}^T\\mathbf{U_K}\\mathbf{\\Lambda_K}^{1/2}=\n",
    "\\mathbf{\\Lambda_K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTX = np.diagflat(1 / ((v_K[:n_PC]))) @ T_KPCA_train.T @ X_train\n",
    "\n",
    "Xr_train = T_KPCA_train @ PTX\n",
    "Xr_test = T_KPCA_test @ PTX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KPCA projection approximates the kernel matrix, so one can check for convergence using the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_approx_train = T_KPCA_train @ T_KPCA_train.T\n",
    "\n",
    "K_test_test = kernel_func(X_test, X_test)\n",
    "K_test_test = k_centerer.transform(K_test_test)\n",
    "K_approx_test = T_KPCA_test @ T_KPCA_test.T\n",
    "\n",
    "table_from_dict(\n",
    "    [\n",
    "        get_stats(\n",
    "            x=X_train,\n",
    "            xr=Xr_train,\n",
    "            y=Y_train,\n",
    "            t=T_KPCA_train,\n",
    "            k=K_train,\n",
    "            kapprox=K_approx_train,\n",
    "        ),\n",
    "        get_stats(\n",
    "            x=X_test,\n",
    "            xr=Xr_test,\n",
    "            y=Y_test,\n",
    "            t=T_KPCA_test,\n",
    "            k=K_test_test,\n",
    "            kapprox=K_approx_test,\n",
    "        ),\n",
    "    ],\n",
    "    headers=[\"Training\", \"Testing\"],\n",
    "    title=\"KPCA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression (KRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the KRR Weights\n",
    "Just as in KPCA, in kernel ridge regression (KRR) we can use the kernel trick to make property predictions [(Saunders 1998)](https://eprints.soton.ac.uk/258942/1/Dualrr_ICML98.pdf). In KRR, the model takes the form\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{Y}}_{KRR} = \\mathbf{K} \\mathbf{P}_{KY},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{P}_{KY}$ is a vector of regression weights. The regression weights can be computed using\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{P}_{KY} = \\left( \\mathbf{K} + \\lambda \\mathbf{I} \\right)^{-1}\\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is the regularization parameter \n",
    "[(Girosi 1995)](https://pdfs.semanticscholar.org/96af/5da062cee7ce50fc0624654c61363506772b.pdf),\n",
    "[(Smola 2000)](https://pdfs.semanticscholar.org/dd09/78a594290f6dc530e65983d79a056874185c.pdf), \n",
    "[(Welling)](https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf). This parameter can be set to some small fraction of the maximum eigenvalue $\\lambda_{max}$ of $\\mathbf{K}$, e.g., $\\lambda = 10^{-16}\\lambda_{max}$, or it can be determined through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max = np.amax(np.linalg.eigvalsh(K_train))\n",
    "regularization = 1e-4\n",
    "lambda_reg = lambda_max * regularization\n",
    "\n",
    "PKY_krr = np.linalg.solve(K_train + lambda_reg * np.eye(n_train), Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our regularization is {{regularization*lambda_max}}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then predict our properties using the training and testing kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_krr_train = K_train @ PKY_krr\n",
    "Y_krr_test = K_test @ PKY_krr\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(\n",
    "    Y_train[:, 0],\n",
    "    Y_krr_train[:, 0],\n",
    "    title=\"Kernel Ridge Regression: Training Set\",\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    **cmaps\n",
    ")\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    Y_krr_test[:, 0],\n",
    "    title=\"Kernel Ridge Regression: Testing Set\",\n",
    "    fig=fig,\n",
    "    ax=axes[1],\n",
    "    **cmaps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR as regression in RKHS\n",
    "\n",
    "It is instructive to see how KRR is equivalent to ridge regression in RKHS features. In other words, the equation for ridge regression is equivalent to the equation for _kernel_ ridge regression when $\\mathbf{X}$ is replaced with $\\mathbf{\\Phi}$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{Y}}_{KRR} = \\mathbf{\\Phi} (\\mathbf{\\Phi}^T\\mathbf{\\Phi} \n",
    "      + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Phi}^T \\mathbf{Y} =\n",
    "   \\mathbf{\\Phi} \\mathbf{\\Phi}^T   (\\mathbf{\\Phi}\\mathbf{\\Phi}^T\n",
    "      + \\lambda \\mathbf{I})^{-1} \\mathbf{Y} = \\mathbf{K} (\\mathbf{K}+\\lambda \\mathbf{I})^{-1} \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "where the second step can be proven through a Taylor expansion, where any function $f(A^T A) * A^T$ can be rewritten as $A^T*f(A A^T)$ by expanding it in the form $\\sum_k c_k (A^T A)^k A^T  = A^T \\sum_k c_k (A A^T)^k$.\n",
    "\n",
    "<!---\n",
    "omes from the relation\n",
    "\\begin{equation}\n",
    "\\left(\\mathbf{B}^T\\mathbf{RB} + \\mathbf{P}^{-1}\\right)^{-1}\\mathbf{B}^T\\mathbf{R}^{-1} = \\mathbf{B}^T \\left(\\mathbf{B}\\mathbf{P}\\mathbf{B}^T + \\mathbf{R}\\right)\n",
    "\\end{equation}\n",
    "whenever $\\mathbf{P}$ and $\\mathbf{R}$ are positive definite. Here, because $\\mathbf{B} = \\mathbf{\\Phi}$, $\\mathbf{P} = \\mathbf{I}$ and $\\mathbf{R} = \\lambda \\mathbf{I}$, the relation holds. [(Matrix Cookbook)](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf), [(Welling)](https://www.ics.uci.edu/~welling/classnotes/papers_class/Kernel-Ridge.pdf)\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "Here our loss function is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell = \\left\\lVert \\mathbf{Y} - \\mathbf{K}\\mathbf{P}_{KY}\\right\\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "which we can compare to that of ridge regression. Note how the additional flexibility afforded by the non-linear kernel halves the MSE. By reducing the regularization, one could reduce the train set error essentially to zero: in fact, the best regularization needs to be determined by cross-validation, or by using an external test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=regularization)\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_lr = lr.predict(X_test)\n",
    "\n",
    "table_from_dict([get_stats(x=X_test, \n",
    "                           yp=Y_lr,\n",
    "                           y=Y_test, \n",
    "                           ), \n",
    "                 get_stats(x=X_test, \n",
    "                           yp=Y_krr_test,\n",
    "                           y=Y_test, \n",
    "                           k=K_test)], \n",
    "                 headers = [\"LR\", \"KRR\"]\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Regularization Parameter\n",
    "\n",
    "Similar to in linear/ridge regression, including some regularization is equivalent to introducing a $\\mathcal{L}^2$ penalty in the loss, associated to $\\left\\lVert \\mathbf{P}_{KY} \\right\\rVert^2$. Here we express it as a fraction of the maximum eigenvalue. While the regularization is used here just to avoid overfitting, it can also be interpreted -- as it is done in the context of Gaussian process regression -- as a measure of the level of noise that is present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizations = np.linspace(-10, -2, 24)\n",
    "krrs = [KernelRidge(alpha=10 ** i, **kernel_params) for i in regularizations]\n",
    "\n",
    "for k in krrs:\n",
    "    k.fit(X=K_train, y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = np.array(\n",
    "    [\n",
    "        np.linalg.norm(Y_test - k.predict(K_test)) ** 2.0\n",
    "        / np.linalg.norm(Y_test) ** 2.0\n",
    "        for k in krrs\n",
    "    ]\n",
    ")\n",
    "best_regularization = 10 ** regularizations[ell.argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even small regularizations lead to an increase in accuracy for predicting out-of-sample data. The best regularization here is {{round(best_regularization, 8)}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(10 ** regularizations, ell, marker=\"o\", zorder=-1)\n",
    "plt.scatter(\n",
    "    best_regularization,\n",
    "    min(ell),\n",
    "    marker=\"o\",\n",
    "    color=\"r\",\n",
    ")\n",
    "\n",
    "plt.xlabel(r\"$\\lambda$\")\n",
    "plt.ylabel(r\"$\\ell$\")\n",
    "\n",
    "plt.title(r\"Effect of $\\lambda$ on $\\ell$ for Kernel Ridge Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCovR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of KernelPCovR\n",
    "Formulating a kernelized version of PCovR is surprisingly simple. One just needs to write formally the expressions using the RKHS features in lieu of the linear features, and combine them into a kernel matrix to obtain the final expression.\n",
    "\n",
    "As an important reminder, $\\tilde{\\mathbf{K}}$ represents the modified Gram matrix used in PCovR, given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{\\mathbf{K}} = \\alpha \\mathbf{XX}^T\n",
    "    + (1 - \\alpha) \\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}}^T,\n",
    "\\end{equation}\n",
    "\n",
    "versus $\\mathbf{K}$, which is our kernel matrix.\n",
    "\n",
    "To construct a PCovR based upon the kernel matrix, we first need to use the approximate $\\hat{\\mathbf{Y}}$ obtained from (possibly regularized) KRR:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{Y}} = \\mathbf{K}\\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{Y},\n",
    "\\end{equation} \n",
    "\n",
    "and to substitute $\\mathbf{K} = \\mathbf{\\Phi \\Phi}^T$ for $ \\mathbf{XX}^T$. \n",
    "\n",
    "We choose to normalize our kernel matrix dividing by the factor  $\\operatorname{Tr}(\\mathbf{K})/N$, so as to obtain a similar balancing of the LR and PCA parts of the loss as is used for linear PCovR. Thus we finally get\n",
    "\n",
    "\\begin{equation}\n",
    "    \\tilde{\\mathbf{K}} = \\alpha \\frac{\\mathbf{K}} {\\operatorname{Tr}(\\mathbf{K})/N} \n",
    "    + (1 - \\alpha) \\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}}^T.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "regularization = 1e-12\n",
    "\n",
    "krr = KernelRidge(alpha=regularization, **kernel_params)\n",
    "krr.fit(K_train, Y_train)\n",
    "Yhat_train = krr.predict(K_train).reshape(-1, Y.shape[-1])\n",
    "PKY = krr.dual_coef_\n",
    "K_pca = K_train / (np.trace(K_train) / n_train)\n",
    "K_lr = Yhat_train @ Yhat_train.T\n",
    "\n",
    "Kt = (alpha * K_pca) + (1.0 - alpha) * K_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting into Latent Space\n",
    "\n",
    "The projection can be obtained as in the linear PCovR case, combining eigenvalues and eigenvectors of $\\tilde{\\mathbf{K}}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{K}\\mathbf{P}_{KT} = \\tilde{\\mathbf{K}}\\mathbf{\\hat{U}_{\\tilde{\\mathbf{K}}}}\\mathbf{\\Lambda_{\\tilde{\\mathbf{K}}}}^{-1/2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_Kt, U_Kt= np.linalg.eigh(Kt)\n",
    "\n",
    "# U_K/v_Ktare already sorted, but in *increasing* order, so reverse them\n",
    "U_Kt= np.flip(U_Kt, axis=1)\n",
    "v_Kt= np.flip(v_Kt, axis=0)\n",
    "\n",
    "U_Kt= U_Kt[:,v_Kt>0]\n",
    "v_Kt= v_Kt[v_Kt>0]\n",
    "\n",
    "# note that Kt might have a large null space\n",
    "print(\"Size {} vs {}\".format(len(v_Kt),len(Kt)))\n",
    "\n",
    "T = Kt @ U_Kt[:, :n_PC] @ np.diagflat(1.0/np.sqrt(v_Kt[:n_PC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given each portion of $\\tilde{\\mathbf{K}}$ contains a factor of $\\mathbf{K}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{K}} = \\left(\\alpha \\frac{\\mathbf{K}} {\\operatorname{Tr}(\\mathbf{K})/N} + (1 - \\alpha)\n",
    "\\mathbf{K}\\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{YY}^T \\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1} \\mathbf{K}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "We can find our projector by dividing $\\tilde{\\mathbf{K}}\\mathbf{\\hat{U}_{\\tilde{\\mathbf{K}}}}\\mathbf{\\Lambda_{\\tilde{\\mathbf{K}}}}^{-1/2}$ by $\\mathbf{K}$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{KT} = \\left(\n",
    " \\alpha\\frac{\\mathbf{I}}{\\operatorname{Tr}{(\\mathbf{K})}/N}\n",
    "+ (1 - \\alpha)\n",
    "\\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1}\\mathbf{YY}^T \\left(\\mathbf{K}+\\mathbf{I}\\lambda\\right)^{-1} \\mathbf{K}\n",
    "\\right)  \\mathbf{\\hat{U}_{\\tilde{\\mathbf{K}}}}\\mathbf{\\Lambda_{\\tilde{\\mathbf{K}}}}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{KT} = \\left(\n",
    " \\alpha\\frac{\\mathbf{I}}{\\operatorname{Tr}{(\\mathbf{K})}/N}\n",
    "+ (1 - \\alpha)\n",
    "\\mathbf{P}_{KY}\\mathbf{\\hat{Y}}^T\n",
    "\\right)  \\mathbf{\\hat{U}_{\\tilde{\\mathbf{K}}}}\\mathbf{\\Lambda_{\\tilde{\\mathbf{K}}}}^{-1/2}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_krr = PKY @ Yhat_train.T\n",
    "\n",
    "P_kpca = np.eye(n_train)/(np.trace(K_train)/n_train)\n",
    "    \n",
    "P = (alpha*P_kpca) + (1.0-alpha)*P_krr\n",
    "PKT = P @ U_Kt[:,:n_PC] @ np.diag(1/np.sqrt(v_Kt[:n_PC])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that this gives the same $\\mathbf{T}$ as above and similarly project the test kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_kpcovr_train = K_train @ PKT\n",
    "T_kpcovr_test = K_test @ PKT\n",
    "\n",
    "print(np.linalg.norm(T-T_kpcovr_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "ref = KernelPCA(n_components=2, **kernel_params)\n",
    "ref.fit(K_train)\n",
    "T_kpca = ref.transform(K_test)\n",
    "\n",
    "plot_projection(\n",
    "    Y_test,\n",
    "    T_kpcovr_test,\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    title=r\"KernelPCovR ($\\alpha={}$)\".format(alpha),\n",
    "    **cmaps\n",
    ")\n",
    "plot_projection(Y_test, T_kpca, fig=fig, ax=axes[1], title=\"KPCA\", **cmaps)\n",
    "\n",
    "fig.suptitle(\n",
    "    r\"These will become increasingly different as $\\alpha \\to 0.0.$\",\n",
    "    y=0.0,\n",
    "    fontsize=plt.rcParams[\"font.size\"] + 6,\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure-property relationship is revealed more clearly in the result of PCovR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Properties \n",
    "\n",
    "Similar PCovR, to get the KRR prediction we should compute $\\mathbf{P}_{TY}$ such that\n",
    "$\\mathbf{Y} = \\mathbf{K}\\mathbf{P}_{KT}\\mathbf{P}_{TY} = \\mathbf{T} \\mathbf{P}_{TY}$. Recall when we derived linear regression, the optimal weights $\\mathbf{P}_{KY}$ were given by the pseudoinverse of our input variable, subject to some regularization. Using this same logic, $\\mathbf{P}_{TY}$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{TY}=(\\mathbf{T}^T\\mathbf{T})^{-1}\\mathbf{T}^T \\mathbf{Y}=\n",
    "\\mathbf{\\Lambda}_{\\tilde{K}}^{-1}\\mathbf{T}^T \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "**Note:** even though $\\mathbf{K}$ must be computed with all the points in the train set, the \n",
    "prediction uses only the projections in the KPCA space, so the latent KernelPCovR variables explain fully \n",
    "structure-property relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTY = np.diagflat(1/((v_Kt[:n_PC]))) @ T_kpcovr_train.T @ Y_train\n",
    "Ypred = K_test @ PKT @ PTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "ref = KernelRidge(alpha=regularization, **kernel_params)\n",
    "ref.fit(K_train, Y_train)\n",
    "yref = ref.predict(K_test)\n",
    "\n",
    "errors = np.concatenate((np.abs(Ypred - Y_test), np.abs(yref - Y_test)))\n",
    "\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    Ypred[:, 0],\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    vmin=errors.min(),\n",
    "    vmax=errors.max(),\n",
    "    title=r\"KernelPCovR ($\\alpha={}$)\".format(alpha),\n",
    "    **cmaps\n",
    ")\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    yref[:, 0],\n",
    "    fig=fig,\n",
    "    ax=axes[1],\n",
    "    vmin=errors.min(),\n",
    "    vmax=errors.max(),\n",
    "    title=\"KRR\",\n",
    "    **cmaps\n",
    ")\n",
    "\n",
    "fig.suptitle(\n",
    "    r\"These will become increasingly different as $\\alpha \\to 1.0.$\",\n",
    "    y=0.0,\n",
    "    fontsize=plt.rcParams[\"font.size\"] + 6,\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of KernelPCovR Performance\n",
    "\n",
    "We will use the utility class to plot how the method is tuned with changing $\\alpha$ and the number of PCA components. Note: executing this cell can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 11\n",
    "alphas = np.linspace(0.0, 1.0, n_alphas)\n",
    "# a = np.linspace(-5.0,5.0,n_alphas)\n",
    "# alphas = np.exp(a)/(np.exp(-a)+np.exp(a))\n",
    "components = np.array([2,3,4,8])\n",
    "n_components = components.size\n",
    "\n",
    "kpcovr_calculators = np.array([[KernelPCovR(mixing=a, n_components=c, \n",
    "                                       kernel='precomputed') for a in alphas] \n",
    "                                                        for c in components])\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    for adx, a in enumerate(alphas):\n",
    "        kpcovr_calculators[cdx][adx].fit(K_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of KernelPCovR Projections and Regressions\n",
    "\n",
    "Just like with PCovR, for KernelPCovR it's useful to get an intuitive sense for the change in the projections and regressions across $\\alpha$, and see the trade-off. Below we plot both projections and regressions for $n_\\alpha$ different values of $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = int(n_alphas ** 0.5)\n",
    "scale = 3\n",
    "\n",
    "t_ref = kpcovr_calculators[0][-3].transform(K_test)\n",
    "\n",
    "pfig, pax = plt.subplots(\n",
    "    n_plots,\n",
    "    int(np.ceil(n_alphas / n_plots)),\n",
    "    figsize=(\n",
    "        scale * int(np.ceil(n_alphas / n_plots)),\n",
    "        scale * n_plots,\n",
    "    ),\n",
    ")\n",
    "\n",
    "rfig, rax = plt.subplots(\n",
    "    n_plots,\n",
    "    int(np.ceil(n_alphas / n_plots)),\n",
    "    figsize=(\n",
    "        scale * int(np.ceil(n_alphas / n_plots)),\n",
    "        scale * n_plots,\n",
    "    ),\n",
    ")\n",
    "for p, r, kpcovr in zip(pax.flatten(), rax.flatten(), kpcovr_calculators[0]):\n",
    "\n",
    "    t = kpcovr.transform(K_test)\n",
    "    y = kpcovr.predict(K_test)\n",
    "\n",
    "    plot_projection(Y_test, check_mirrors(t, t_ref), fig=pfig, ax=p, **cmaps, alpha=1.0)\n",
    "\n",
    "    plot_regression(Y_test[:, 0], y[:, 0], fig=pfig, ax=r, **cmaps, alpha=1.0)\n",
    "\n",
    "    p.set_title(r\"$\\alpha=$\" + str(round(kpcovr.mixing, 6)))\n",
    "    r.set_title(r\"$\\alpha=$\" + str(round(kpcovr.mixing, 6)))\n",
    "\n",
    "\n",
    "for p, r in zip(pax.flatten()[n_alphas:], rax.flatten()[n_alphas:]):\n",
    "    p.axis(\"off\")\n",
    "    r.axis(\"off\")\n",
    "\n",
    "pfig.subplots_adjust(wspace=0.6, hspace=0.6)\n",
    "pfig.suptitle(r\"Projections across $\\alpha$\")\n",
    "rfig.subplots_adjust(wspace=0.6, hspace=0.6)\n",
    "rfig.suptitle(r\"Regressions across $\\alpha$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of KernelPCovR Loss Terms\n",
    "\n",
    "Computing the loss minimised by KernelPCovR is trivial if one computes explicitly a RKHS approximation of $\\mathbf{\\Phi}$, but it requires some work if one wants to avoid evaluating $\\mathbf{\\Phi}$\n",
    "\n",
    "Rewriting in terms of the RKHS, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell = \\alpha\\lVert \\mathbf{\\Phi} - \\mathbf{\\Phi}\\mathbf{P}_{\\Phi T}\\mathbf{P}_{T\\Phi}\\rVert^2 + (1 - \\alpha)\\left\\lVert \\mathbf{Y} - \\mathbf{KP}_{KT}\\mathbf{P}_{TY}\\right\\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "In case one wants to avoid evaluating the RKHS, however, $\\ell_\\text{proj}$ may be computed in terms of the kernel.\n",
    "\n",
    "Indicating the kernel between set $A$ and $B$ as $\\mathbf{K}_{AB}$, the projection of set $A$ as $\\mathbf{T}_A$, and with N and V as the train and validation/test set, one obtains\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\ell_\\text{proj}=&\n",
    "\\operatorname{Tr}\\left[\n",
    "\\mathbf{K}_{VV} - 2\n",
    "    \\mathbf{K}_{VN} \\mathbf{T}_N (\\mathbf{T}_N^T \\mathbf{T}_N)^{-1}  \\mathbf{T}_V^T\\right.\\\\\n",
    "    +&\\mathbf{T}_V(\\mathbf{T}_N^T \\mathbf{T}_N)^{-1}  \\mathbf{T}_N^T   \\mathbf{K}_{NN} \\left.\\mathbf{T}_N (\\mathbf{T}_N^T \\mathbf{T}_N)^{-1}    \\mathbf{T}_V^T.\n",
    "\\right]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "When the loss is evaluated on the train set, so that $N\\equiv V$, this expression reduces to\n",
    "\\begin{equation}\n",
    "      \\ell_\\text{proj} = \\operatorname{Tr}\\left(\\mathbf{K}_{NN} - \\mathbf{K}_{NN} \\mathbf{P}_{KT} \\mathbf{P}_{TK} \\right).\n",
    "\\end{equation}\n",
    "where $\\mathbf{P}_{TK} = (\\mathbf{T}_N^T \\mathbf{T}_N)^{-1} \\mathbf{T}_N^T \\mathbf{K}_{NN}$.\n",
    "\n",
    "Where $\\operatorname{Tr}{(\\mathbf{K}_{VV})} \\neq 1$, it is necessary to divide the preceding loss equation by $\\operatorname{Tr}{(\\mathbf{K}_{VV})}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_vv = K_test_test.copy()\n",
    "K_vn = K_test.copy()\n",
    "K_nn = K_train.copy()\n",
    "\n",
    "\n",
    "def l_proj(my_kpcovr):\n",
    "    t_n = my_kpcovr.transform(K_train)\n",
    "    t_v = my_kpcovr.transform(K_test)\n",
    "\n",
    "    w = t_n @ np.linalg.pinv(t_n.T @ t_n) @ t_v.T\n",
    "    return np.trace(K_vv - 2 * K_vn @ w + w.T @ K_nn @ w) / np.trace(K_vv)\n",
    "\n",
    "\n",
    "def l_y(my_kpcovr):\n",
    "    return (\n",
    "        np.linalg.norm(Y_test - my_kpcovr.predict(K_test)) ** 2.0\n",
    "        / np.linalg.norm(Y_test) ** 2.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_proj = np.zeros((n_components, n_alphas))\n",
    "L_regr = np.zeros((n_components, n_alphas))\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    for adx, a in enumerate(alphas):\n",
    "        L_proj[cdx, adx], L_regr[cdx, adx] = (\n",
    "            l_proj(kpcovr_calculators[cdx][adx]),\n",
    "            l_y(kpcovr_calculators[cdx][adx]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [axs_regr, axs_proj] = plt.subplots(1, 2, figsize=(12, 6), sharex=True)\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    axs_regr.plot(alphas, L_regr[cdx, :], marker=\"o\", label=\"{:d} PCs\".format(c))\n",
    "    axs_proj.plot(alphas, L_proj[cdx, :], marker=\"o\", label=\"{:d} PCs\".format(c))\n",
    "\n",
    "axs_regr.set_ylabel(r\"$\\ell_{regr}$\")\n",
    "axs_regr.set_xlabel(r\"$\\alpha$\")\n",
    "axs_proj.set_ylabel(r\"$\\ell_{proj}$\")\n",
    "axs_proj.set_xlabel(r\"$\\alpha$\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "axs_regr.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing KernelPCovR, the total error is reduced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "axsLoss = fig.add_subplot(1, 2, 1)\n",
    "axsSum = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    axsLoss.loglog(\n",
    "        L_proj[cdx, :], L_regr[cdx, :], marker=\"o\", label=\"{:d} PCs\".format(c)\n",
    "    )\n",
    "\n",
    "axsLoss.set_xlabel(r\"$\\ell_{proj}$\")\n",
    "axsLoss.set_ylabel(r\"$\\ell_{regr}$\")\n",
    "axsLoss.legend()\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    loss_sum = L_regr[cdx, :] + L_proj[cdx, :]\n",
    "    axsSum.semilogy(alphas, loss_sum, marker=\"o\", label=\"{:d} PCs\".format(c))\n",
    "    print(\"Optimal alpha for {:d} PCs = {:.2f}\".format(c, alphas[np.argmin(loss_sum)]))\n",
    "\n",
    "axsSum.set_xlabel(r\"$\\alpha$\")\n",
    "axsSum.set_ylabel(r\"$\\ell_{regr} + \\ell_{proj}$\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Sparse Kernel Methods\n",
    "\n",
    "Continue on to the [next notebook](4_SparseKernelMethods.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in `scikit-learn`\n",
    "\n",
    "Classes from `sklearn` enable computing KPCA and KRR. Here we'll review the basics of their usage; readers are encouraged to refer to sklearn documentation for further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set kernel = 'precomputed' and supply kpca.fit(X) with X = K_train to improve computational efficiency.\n",
    "kpca = KernelPCA(n_components=2, **kernel_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpca.fit(K_train)` will generate the kernel for $\\mathbf{X}$ and compute/internally store the eigenvectors/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca.fit(K_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpca.transform(K)` will compute and return the KPCA projection $\\mathbf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_KPCA_train = kpca.transform(K_train)\n",
    "T_KPCA_test= kpca.transform(K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "plot_projection(\n",
    "    Y_train, T_KPCA_train, title=\"KPCA of Training Data\", fig=fig, ax=axes[0], **cmaps\n",
    ")\n",
    "plot_projection(\n",
    "    Y_test, T_KPCA_test, title=\"KPCA of Testing Data\", fig=fig, ax=axes[1], **cmaps\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTX = np.diagflat(1 / ((v_K[:n_PC]))) @ T_KPCA_train.T @ X_train\n",
    "\n",
    "Xr_train = T_KPCA_train @ PTX\n",
    "Xr_test = T_KPCA_test @ PTX\n",
    "\n",
    "K_approx_train = T_KPCA_train @ T_KPCA_train.T\n",
    "K_test_test = kernel_func(X_test, X_test)\n",
    "K_approx_test = T_KPCA_test @ T_KPCA_test.T\n",
    "\n",
    "table_from_dict(\n",
    "    [\n",
    "        get_stats(\n",
    "            x=X_train,\n",
    "            xr=Xr_train,\n",
    "            y=Y_train,\n",
    "            t=T_KPCA_train,\n",
    "            k=K_train,\n",
    "            kapprox=K_approx_train,\n",
    "        ),\n",
    "        get_stats(\n",
    "            x=X_test,\n",
    "            xr=Xr_test,\n",
    "            y=Y_test,\n",
    "            t=T_KPCA_test,\n",
    "            k=K_test_test,\n",
    "            kapprox=K_approx_test,\n",
    "        ),\n",
    "    ],\n",
    "    headers=[\"Training\", \"Testing\"],\n",
    "    title=\"KPCA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set kernel = 'precomputed' and supply krr.fit(X,y) with X = K_train to improve computational efficiency.\n",
    "krr = KernelRidge(alpha=regularization, **kernel_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `krr.fit(X,Y)` will compute the weights $\\mathbf{P}_{KY}$ and internally store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krr.fit(K_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `krr.predict(X)` will generate the desired kernel from input $\\mathbf{X}$ and compute and return the predicted $\\mathbf{Y}$ values from $\\hat{\\mathbf{Y}}_{KRR} = \\mathbf{K}\\mathbf{P}_{KY}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_krr_train = krr.predict(K_train)\n",
    "Y_krr_test = krr.predict(K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(\n",
    "    Y_train[:, 0],\n",
    "    Y_krr_train[:, 0],\n",
    "    title=\"Kernel Ridge Regression of Training Data\",\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    **cmaps\n",
    ")\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    Y_krr_test[:, 0],\n",
    "    title=\"Kernel Ridge Regression of Testing Data\",\n",
    "    fig=fig,\n",
    "    ax=axes[1],\n",
    "    **cmaps\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([get_stats(x=X_train, yp=Y_krr_train,y=Y_train, k=K_train), \n",
    "                 get_stats(x=X_test,  yp=Y_krr_test, y=Y_test, k=K_test)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"Kernel Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in `skcosmo`\n",
    "\n",
    "Classes from `skcosmo` enable computing KernelPCovR. Here we'll review the basics of their usage; readers are encouraged to refer to skcosmo documentation for further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skcosmo.decomposition import KernelPCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***To avoid confusion with scikit-learn naming conventions, where `alpha` is a regularization parameter, the keyword `mixing` is used to specify the KernelPCovR $\\alpha$.***\n",
    "\n",
    "We will show the function signature using X, but we will also supply our precomputed kernel for computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KernelPCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = KernelPCovR(mixing=0.8, n_components=2, **kernel_params, center=True, alpha=1E-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `kpcovr.fit(X, Y, K)` will compute the weights $\\mathbf{P}_{KY}$ and internally store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.mixing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kpcovr.transform(X)` returns the projection $\\mathbf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = kp.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pcovr.predict(X)` returns the regressed properties $\\mathbf{Y}_p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "plot_projection(Y_test, t, title=\"KernelPCovR\", fig=fig, ax=ax[0], **cmaps)\n",
    "plot_regression(Y_test[:, 0], y[:, 0], title=\"KernelPCovR\", fig=fig, ax=ax[1], **cmaps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
